\documentclass[12pt, oneside]{amsart}

\usepackage{amssymb}
\usepackage{cleveref}
\usepackage[backend=bibtex]{biblatex}
\usepackage{mathtools}
\addbibresource{bibliography.bib}

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}

\theoremstyle{remark}
\newtheorem{remark}[thm]{Remark}

\numberwithin{equation}{section}

\include{macros}

\begin{document}
\title{Least Squares and Smolyak's Algorithm}


\author{Jakob Eggl\(^\star\)}
\address{Institute of Analysis, Johannes Kepler University Linz, Austria.}
\email{jakob.eggl@jku.at}

\author{Elias Mindlberger\(^\star\)}
\address{Institute of Analysis, Johannes Kepler University Linz, Austria.}
\email{elias.mindlberger@jku.at}

\author{Mario Ullrich}
\address{Institute of Analysis, Johannes Kepler University Linz, Austria.}
\email{mario.ullrich@jku.at}
\thanks{\(\star\) Equal Contribution.}


\begin{abstract}
We present novel, large-scale experiments for polynomial interpolation in high dimensional settings using some of the most popular algorithms available. We compare Smolyak's Algorithm (SA) on sparse grids and Least Squares (LSQ) using random data. We empirically confirm that interpolation using LSQ performs equally as good on smooth and better on non-smooth functions if SA is given \(n\) and LSQ is given \(\cO(n \log n)\) points.
\end{abstract}


\maketitle
\thispagestyle{empty}


\tableofcontents

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Smolyak's Algorithm on Sparse Grids has been of high theoretical and practical interest for a long time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


We use \({[N]}_0 \defn \set{0, 1, \dots, N}\) and \([N]\) for the set \(\set{1, \dots, N}\). For the set of polynomials \(p: V \to W\) of maximal degree \(N\) and sets \(V, W \subseteq \C^K,\ K \in \N\) we use the notation \(p \in \pols[N]{V, W}\). We use \(f \propto g\) for functions \(f, g\) to denote \(f = c g\) for a constant \(c \in \R\) and \(f \lesssim g\) to denote \(f \leq c g\). With \(B(X)\) we denote the closed unit ball of a normed space \(X\), i.e.\ \(B(X) \defn \set{x \in X: \norm[X]{x} \leq 1}\).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Smolyak's Algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Smolyak's Algorithm is determinstic and can be used in various settings. These include but are not limited to
\begin{enumerate}
    \item function interpolation,
    \item numerical integration,
    \item solving ODEs and PDEs,
    \item \(\cdots\)
\end{enumerate}
\subsection*{Interpolation in one dimension}
In the onedimensional setting it is well known that for any collection of points \(\ZZ \defn \set{z_n}_{n=0}^N \subseteq \R\) and known function values \(f(z_n): n \in {[N]}_0\) for \(f: \R \to \R\) there is always a unique interpolating polynomial \(i\), i.e. \(i \in \pols[N]{\R, \R}: i(z_j) = f(z_j)\) s.t.\ \(j \in {[N]}_0\). To find the polynomial \(i\), one can, among other available methods, choose to perform Lagrange Interpolation which yields an explicit construction of the form
\begin{equation}\label{eq:lagrangeInterpolation}
    \mathcal{I}\left(\FF\right)(x) = \sum_{n=0}^N f(z_n) \ell_n(x)\quad \text{ s.t. }\quad \ell_n(x) \defn \prod_{\substack{j = 0\\n \neq j}}^N \frac{x-z_j}{z_n-z_j}
\end{equation}
where \(\FF \defn \set{\left[z_n, f\left(z_n\right)\right]}_{n=1}^{N}\). We set \(i \defn \cI(\FF)\). Note that \(\cI\) is a mapping from data, which may be seen as arranged in a \(2 \times N\) matrix, to an \(N\)-degree polynomial. Thus \(\cI: \R^{2 \times N} \to \pols[N]{\R,\R}\). since each basis function \(\ell_n(x)\) is a polynomial of degree \(N\). Thus, if \(f \in \pols[N]{\R, \R}\), one obtains \(i = f\). Moreover, \(i\) is interpolating since \(\ell_n(z_j) = \delta_{n}(j)\). Even though (\ref{eq:lagrangeInterpolation}) has nice theoretical properties, it is suboptimal for practical settings due to the following facts. (1) If a node \(z_n\) is close to a node \(z_j\) such that \(n \neq j\), computing the product \(\ell_n(x)\) becomes numerically unstable. (2) The addition of new data \(\left(z, f(z)\right)\) requires to recalculate all basis functions anew in \(\cO(N^2)\) time. (3) To compute \(i(x)\) in this form requires computational complexity of \(\cO(N^2)\). This does not mean that one has to abandon Lagrange Interpolation at once. We may write \cref{eq:lagrangeInterpolation} in a much more stable way as
\begin{equation}\label{eq:barycentricLagrange}
    i(x) \defn \cI(\FF)(x) = \frac{\sum_{n=0}^N \frac{w_n}{x-z_n} f(z_n)}{\sum_{n=0}^N \frac{w_n}{x-z_n}}
\end{equation}
for \(w_n \in \R : n \in {[N]}_0\). If the weights \(w_n\) are known, computing \(i(x)\) in this form admits a complexity of \(\cO(N)\). Luckily, the weights have closed-form, analytic solutions for many deterministic point sets used in practice and are hence considered to be computable in \(\cO(1)\) time. Thus, adding a new data pair \((z, f(z))\) requires total complexity of \(\cO(N)\) for the recomputation of said weights. The general identity \[
    w_n = \frac{1}{\prod_{\substack{j=0\\j \neq n}}^N x_n - x_j} = \frac{1}{\ell'(x_n)}
\]
always holds. To specify the above, consider the set of \emph{equidistant} nodes \(\set{z_n \defn 2/n} \subset [-1, 1]\). Then \begin{equation}\label{eq:weightsEquidistant}
    w_n = {\left(-1\right)}^n {N \choose n}.
\end{equation}
For large \(N\), \cref{eq:weightsEquidistant} is problematic since weights \(w_j, w_n: n \neq j\) now may vary by factors as large as \(\cO\left(2^N\right)\). This means that interpolation in equispaced points is susceptible to the \emph{Runge phenomenon}. For this interpolation problem to be well posed one should use point sets with asymptotic density \(\rho(x) \propto 1/\sqrt{1-x^2}\). Many point sets admit this asymptotic density, for example the Chebyshev points of the first kind are given as
\begin{equation}\label{eq:chebyPts1}
    \ZZ_C^{(1)}(N) \defn \set{z^{(1)}_n \defn \cos \frac{\left(2n+1\right) \pi}{2n+2} \mid n \in {[N]}_0},
\end{equation}
similarly, the Chebyshev points of the second kind are
\begin{equation}\label{eq:chebyPts2}
    \ZZ_C^{(2)}(N) \defn \set{z^{(2)}_n \defn \cos \frac{n\pi}{N} \mid n \in {[N]}_0}.
\end{equation}
In either of these cases, the weights \(w_n\) have admit nice closed forms as \(w_n^{(k)}: k \in \set{1,2}\) dependent on which point set is in use as
\begin{equation}\label{eq:weightsChebyshev}
    w_n^{(1)} = {\left(-1\right)}^n \sin \frac{\left(2n+1\right) \pi}{2n+2}, \quad w_n^{(2)} = {\left(-1\right)}^n \cdot \begin{cases}
        1/2 &n \in \set{0, N}\\
        1 &\text{else.}
    \end{cases}
\end{equation}

\subsection*{Interpolation in \(d\) dimensions} In principle, one may just \emph{extend} one dimensional interpolation rules to \(d\) dimensions by forming the tensor product of all interpolation rules. To specify, take the one dimensional algorithm
\begin{equation}\label{eq:general1dInterpolation}
    \cI^{(N)}(\FF(N))(x) = \sum_{n = 1}^{m(N)} b_{n}(x) f(z_n)
\end{equation}
such that \(\cI^{(N)}(\FF(N))\) is interpolating on the data \(\FF(N) \defn \set{\left[z_n, f\left(z_n\right)\right]}_{n=1}^{m(N)}\) with basis functions \(b_n \in C[0, 1]\) such that \(b_n(z_j) = \delta_n(j)\). To obtain a set of multidimensional point sets from one dimensional rules like\ \cref{eq:chebyPts1} or\ \cref{eq:chebyPts2}, one uses the usual cartesian product
\begin{equation}\label{eq:cartProduct}
    \ZZ(N, d) \defn \ZZ(N_1) \times \cdots \times \ZZ(N_d) \defn \bigcup_{j=1}^d \bigcup_{n_j=1}^{m(N_j)} \set{\left(z_{n_1}, \dots, z_{n_d}\right)}
\end{equation}
where \(z_n \defn \left(z_{n_1}, \dots, z_{n_d}\right)\) and \(N \defn \left(N_1, \dots, N_d\right) \in \N^d\) is the usual multi-index. Using the tensor product formula, one obtains the very general expression
\begin{equation}\label{eq:tensorProdInterp}
    {\left( \cI^{(N_1)} \otimes \cdots \otimes \cI^{(N_d)} \right)}(\FF)
    = \sum_{n_1 = 1}^{m(N_1)} \cdots \sum_{n_d = 1}^{m(N_d)} \left( b_{n_1} \otimes \cdots \otimes b_{n_d} \right) f(z_{n})
\end{equation}
where \(\FF \defn \bigcup_{j=1}^d \FF(N_j)\). In this case, the tensor product interpolation is a mapping of the form
\begin{equation}\label{eq:tensorProdInterpMappingSpecification}
    \bigotimes_{j=1}^d \cI^{(N_j)}: \R^{(d+1) \times \left(\sum_{j=1}^d m(N_j) \right)} \to \pols[\left(\max \set{m(N_j)} \right)]{\R^d, \R}.
\end{equation}
We denote \(\cI^{(N)} \defn \bigotimes_{j=1}^d \cI^{(N_j)}\). If the basis functions \(b_n\) are scalar-valued, their tensor product is just
\begin{equation}\label{eq:tensorProdFunc}
    \left( \bigotimes_{n=1}^d b_n \right)(x_1, \dots, x_d) \defn \prod_{n \in [d]} b_n(x_n).
\end{equation}
In Smolyak's Algorithm, the function \(m: \N \to \N\) is used as a growth rule specifying how many points are to be used by the interpolant. Specifically, we will use a doubling rule such that \[
    m(n) \defn \begin{cases}
        2^{n-1}+1 &n > 1\\
        1 &\text{else}.
    \end{cases}
\]
Clearly, the usual representation of the Lagrange polynomial (\ref{eq:lagrangeInterpolation}) fits this framework. For the barycentric form, one obtains the \(d\)-variate interpolation
\begin{equation}\label{eq:multivarBarycentric}
    \cI(\FF)(x) \defn \frac{\sum_{n_1=1}^{m(N_1)} b^{(1)}_{n_1}(x_1) \cdots \sum_{n_d=1}^{m(N_d)}b^{(d)}_{n_d}(x_d) f(z_{n_1}, \dots, z_{n_d})}{\sum_{n_1=1}^{m(N_1)} b^{(1)}_{n_1}(x_1) \cdots \sum_{n_d=1}^{m(N_d)}b^{(d)}_{n_d}(x_d)}
\end{equation}
where
\begin{equation}\label{eq:barycentricBasis}
    b^{(k)}_{n_k}(x) \defn \frac{w_{n_k}}{x-z_{n_k}} \mid k \in [d].
\end{equation}
If a rule yielding admissible point sets, like\ Chebyshev's rules (\ref{eq:chebyPts1} or\ \ref{eq:chebyPts2}), is available, the only thing left to do is to specify how many nodes \(N_j\) (or \(m(N_j)\)) to pick in each direction. If this number is large simultaneously for all dimensions,\ \cref{eq:tensorProdInterp} becomes intractable very quickly. Smolyak's algorithm defines a resolution-like number \(q \in \N\) to specify how close-meshed points should be picked. It proceeds by taking all multi-indices \(N\) with \(1\)-norm less than or equal to \(q\) and specifies to interpolate with the algorithm
\begin{equation}\label{eq:smolyak}
    \cA(q, d)\left(\XX\right) \defn \sum_{\norm[1]{N} \leq q} \left(\cI^{(N_1)} \otimes \cdots \otimes \cI^{(N_d)}\right)(\XX(N)).
\end{equation}
Note that, with this construction \(q \geq d\) is necessary. The set \(\XX\) in this case is
\begin{equation}\label{eq:indexGrid}
    \XX(N) \defn \bigcup_{n \in [d]} \FF(N_n)
\end{equation}
All points used by this algorithm are then
\begin{equation}\label{eq:sparseGrid}
    \XX \defn \XX^\leq(q) \defn \bigcup_{\norm[1]{N} \leq q} \XX(N).
\end{equation}
We call \(\XX\) a \emph{sparse grid}. We use \(\XX^=(q)\) for denoting the corresponding set replaced with the rule that it takes all vectors \(N: \norm[1]{N} = q\). In case the point set is nice, for example for Chebyshev points, one obtains a \emph{nesting} of sparse grids for increasing fineness scales: \(\XX^\leq(q) \subset \XX^\leq(q+1)\). This construction has the advantage that the practicioner does not need to save the whole grid \(\XX\) at once. It is enough to use the refinements of each level-increase \(q \mapsto q+1\) and interpolate at each such level separately, each time discarding the old values. We may thus define the difference operator
\begin{equation}\label{eq:differenceOp}
    \Delta^{(q)} \defn \left( \bigotimes_{\norm[1]{N}=q+1} \cI^{(N)} \right) - \left( \bigotimes_{\norm[1]{N} = q} \cI^{(N)} \right)
\end{equation}
Now, Smolyak's algorithm can be written as
\begin{equation}\label{eq:SmolyakWithDifferences1}
    \cA(q, d) \defn \sum_{\norm[1]{N} \leq q} \Delta^{(q)}(\XX(N))
\end{equation}
or equivalently
\begin{equation}\label{eq:SmolyakWithDifferences2}
    \cA(q, d) = \cA(q-1, d) + \sum_{\norm[1]{N}=q} \cI^{(N)}(\XX(N))
\end{equation}
with \(\cI^{(0)} = 0\).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Least Squares}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Contrary to the construction of exactly interpolating approximants in the case of Smolyak's algorithm, least squares is a conceptually simpler algorithm. We are given the overdetermined system \[
    Vz = y
\]
where \(V \in \R^{n \times m}\) with \(n > m\). It is well--known that this system may be inconsistent and no exact solution exists. However, one may always pose the optimisation problem solving for \(z \in \C^{m}\) with the smallest error
\begin{equation}\label{eq:leastSquares}
    \inf_{z \in \R^{m}} \norm[]{Vz - y}.
\end{equation}
It is further known that, in case of a full--rank matrix \(V\), the unique solution to \cref{eq:leastSquares} is given by \[
    z^\star = \left(V^\top V\right)^{-1} V^\top y.
\]

In our specific case of polynomial interpolation, \(V\) is the Vandermonde matrix, consisting of basis polynomials \(b_1, b_2, \dots, b_m\) evaluated at the sampled points \(x_1, x_2, \dots, x_n\) in \(\Omega \subseteq \R^d\) and \(y\) is a vector of function values sampled from the unknown function \(f: \Omega \to \R\), i.e.\ \(y = \left(f(x_j)\right)_{j=1}^n\). In this case, the Vandermonde matrix is never singular as long as the sampled points do not overlap.
Hence, the solution to the approximation problem \[
    \inf_{p} \norm[]{f-p}
\]
can analytically be expressed as \[
    p^\star: \Omega \to \R, t \mapsto \sum_{j=1}^m z_j^\star b_j(t)
\]
where \(z^\star = \left(z_j\right)_{j=1}^m\) is given by \cref{eq:leastSquares}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical Guarantees}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The notation in the following is borrowed from \cite{Ullrich_2020}.
In this section we introduce a formal setting to the former considerations. That is, we consider a Hilbert space \(H\) of real-valued functions on a set \(D\) such that point evaluation \[
    \delta_x: f \mapsto \int_D f\ \d \delta_x = f(x)
\]
is a bounded, linear functional on \(H\).
The general formulation of least squares allows for a broad class of recovery problems. In our specific case, the function recovery of real--valued functions on a \(d\)--dimensional (compact) subset \(D\) using basis functions of a \(k\)--dimensional subspace \(V_k \defn \spn \set{b_1, \dots, b_k}\), we consider the specific form of least squares, given by\[
    A_{n, k}(f) \defn \argmin_{g \in V_k} \sum_{i=1}^n \frac{\abs{g(x_i) - f(x_i)}^2}{\varrho_k(x_i)}
\]
where \[
    \varrho_k(x) = \frac{1}{2} \left( \frac{1}{k} \sum_{j < k} b_{j+1}(x)^2 + \frac{1}{\sum_{j \geq k} a_j^2} \sum_{j \geq k} a_j^2 b_{j+1}(x)^2 \right)
\]
and \(x_1, \dots, x_n \in D\). Whenever \(f \in V_k\), then, of course, \(f = A_{n, k}(f)\). With
\begin{equation}\label{eq:worstCaseError}
    e\left(A_{n,k}, H\right) \defn \sup_{f \in B(H)} \norm[L_2]{f - A_{n,k}(f)},
\end{equation}
we denote the worst case error of \(A_{n,k}\), where we measure the error of the reconstruction in the space \(L_2 \defn L_2(D, \Sigma, \mu)\) of square integrable functions on \(D\) with respect to the measure \(\mu\), such that \(H\) is embedded into \(L_2\). In light of this, the \(n\)--th minimal error is denoted by \[
    e_n(H) \defn \inf_{\substack{x_1, \dots, x_n \in D \\ \phi_1, \dots, \phi_n \in L_2}} \sup_{f \in B(H)} \norm[L_2]{f - \sum_{i=1}^n f(x_i) \phi_i},
\]
and can be understood as the worst case error of the optimal algorithm using \(n\) function values. We get the clear inequality \(e_n(H) \leq e(A_{n,k}, H)\) for any point set \(\set{x_1, \dots, x_n}\). With \[
    a_n(H) := \inf_{\substack{h_1^\star, \dots, h_n^\star \in H^\star \\ \phi_1, \dots, \phi_n \in L_2}} \sup_{f \in B(H)} \norm[L_2]{f - \sum_{i=1}^n h_i^\star(f) \phi_i}
\]
we denote the \(n\)-th approximation number, which is the worst-case error of an optimal algorithm that uses the \(n\) best arbitrary linear and bounded functionals as information about the unknown. This quantity is equal to the \(n\)-th singular value of the embedding \(\id: H \to L_2\).

The following is known since \cite{Krieg_2020}.
\begin{thm}[Krieg--Ullrich]\label{thm:kriegUllrich2020}
    There exist constants \(C, c > 0\) and a sequence of natural numbers \((k_n)\) with each \(k_n \geq cn/\log(n+1)\) and for any \(n \in \N\) and measure space \(D, \Sigma, \mu\), and any RKHS \(H\) of real-valued functions on \(D\) embedded into \(L_2(D, \Sigma, \mu)\), we have \[
        e_n(H) \leq \sqrt{\frac{C}{k_n} \sum_{j \geq k_n} a_j(H)^2}.
    \]
\end{thm}
In particular, for 
\begin{equation}\label{eq:orderOfApproximationNumbers}
    a_n(H) \lesssim n^{-s} \log^{\alpha + s}(n)
\end{equation}
with \(s > 1/2, \alpha \in \R\), this implies \[
    e_n(H) \lesssim n^{-s} \log^{\alpha+s}(n).
\]
The following follows from \cite{Ullrich_2020}.
\begin{thm}[Ullrich]
    Given \(n \geq 2\) and \(c > 0\), let \[
        k_n := \floor*{\frac{n}{2^8 (2+c) \log n}},
    \]
    then, for any measure space \(D, \Sigma, \mu\) and any RKHS \(H\) of real-valued functions on \(D\), embedded into \(L_2(D, \Sigma, \mu)\), it holds that \[
        e_n\left(A_{n, k_n}, H\right) \leq \sqrt{ \frac{2}{k_n} \sum_{j > k_n} a_j(H)^2 }
    \]
    with probability at least \(1-8n^{-c}\).
\end{thm}

\subsection*{Examples}
In particular, \ref{eq:orderOfApproximationNumbers} is satisfied for the approximation numbers on the Sobolev space of dominating mixed smoothness, \begin{align*}
    H &\defn H_{\text{mix}}^s\left(\T^d\right)\\
    &\defn \set{f \in L_2\left(\T^d\right) \mid \norm[H]{f}^2 \defn \sum_{m \in \N_0^d} \prod_{j=1}^d (1+\abs{m_j}^{2s}) \inner{f}{b_m}^2_{L_2} < \infty}
\end{align*}
where \(b_m \defn \otimes_{j=1}^d b_{m_j}^{(1)}\) and \(m = (m_1, \dots, m_d)\) with \begin{align*}
    b_{2m}^{(1)} &\defn \sqrt{2}\cos(2\pi m x)\\
    b_{2m-1}^{(1)} &\defn \sqrt{2}\sin(2\pi m x)
\end{align*}
and \(b_0^{(1)} := 1\). This satisfies the assumption for \(s > 1/2\). In particular, we can say
\begin{equation}\label{eq:conclusio}
    e_n\left( H_{\text{mix}}^s\left(\T^d\right) \right) \lesssim n^{-s}\log^{sd}(n)
\end{equation}
whenever \(s > 1/2\). This disproves a previously posted conjecture (Conjecture 5.26) in \cite{Dung_Temlyakov_Ullrich_2018} and shows that Smolyak's algorithm is not optimal in this case. The surprising fact is that, despite an optimal, deterministic construction of the point sets used for reconstruction being unknown, random i.i.d. points suffice for a reconstruction error that is on the order of optimal points, with probability tending to \(1\). We verify this by our experimental findings, presented in \ref{sec:experimentalFindings} with a much better relative number of points used for LSQ function recovery vs. SA recovery than guaranteed in this section, i.e. better constants than explicitly known before. It remains an open problem to rigorously improve upon the constants in \ref{eq:conclusio}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Findings}\label{sec:experimentalFindings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage

\nocite{*}
\printbibliography

\end{document}
