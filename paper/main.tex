% Magic Comments
% !TeX spellcheck = en_GB
% !TeX encoding = utf8
% !TeX program = pdflatex

\documentclass[12pt, oneside]{amsart}

\usepackage{amssymb}
%\usepackage[%
%	backend=biber,
%	style=numeric-comp, % authoryear
%	bibstyle=numeric, % authoryear
%	citestyle=numeric, % authoryear
%	% maxcitenames=2,
%	sorting=none, %nty
%	backref=true,
%	backrefstyle=none
%]{biblatex}
\usepackage[style=numeric-comp, backend=biber, sorting=nty, doi=false, 
isbn=false, natbib=true]{biblatex} % set bibliography = bibtex in TexStudio
\addbibresource{bibliography.bib}
\emergencystretch=1em % To remove error with bad box

\usepackage{mathtools}
\usepackage{xcolor}
\usepackage[breaklinks=true]{hyperref}
\usepackage{cleveref}

\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[thm]{Definition}

\theoremstyle{remark}
\newtheorem{remark}[thm]{Remark}

\numberwithin{equation}{section}

\include{macros}

% %%%%%%%%%%%%% %
% General TODOs %
% TODO: Cite Barthelmann, Ritter, Novak
% TODO: 


\begin{document}
\title{Least Squares and Smolyak's Algorithm} % TODO: Add small title in 
%[]-Brackets


% With the following definition of the authors, we don't have the unnecessary 
%extra comma before "And Mario 
% Ullrich"
\author{Jakob Eggl, Elias Mindlberger and Mario Ullrich}
\date{\today}
\keywords{Polynomial interpolation, Sparse-Grids, Least-Squares, Smolyak} % 
%TODO: Add or Adapt Keywords


%\author{Jakob Eggl}
%\address{Institute of Analysis, Johannes Kepler University Linz, Austria.}
%\email{jakob.eggl@jku.at}
%
%\author{Elias Mindlberger}
%\address{Institute of Analysis, Johannes Kepler University Linz, Austria.}
%\email{elias.mindlberger@jku.at}
%
%\author{Mario Ullrich}
%\address{Institute of Analysis, Johannes Kepler University Linz, Austria.}
%\email{mario.ullrich@jku.at}

% \thanks{\(\star\) Equal Contribution.}


\begin{abstract}
We present novel, large-scale experiments for polynomial interpolation in high 
dimensional settings using some of the most popular algorithms available. We 
compare Smolyak's Algorithm (SA) on sparse grids and Least Squares (LSQ) using 
random data. We empirically confirm that interpolation using LSQ performs 
equally as good on smooth and better on non-smooth functions if SA is given 
\(n\) and LSQ is given \(\cO(n \log n)\) points.\newline
Code available at: 
\url{https://github.com/th3lias/NumericalExperiments}
% TODO: Make repository public or maybe change owner
\end{abstract}

\maketitle

%\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Smolyak's Algorithm on Sparse Grids has been of high theoretical and practical 
interest for a long time. % TODO: Make this longer


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TODO: Probably rewrite a bit

We denote the indexset containing all indices $i\in \Z$ 
where $m\leq i\leq n$ for $m\leq n$ with \(\indexset[m]{n}\). For the set of 
polynomials \(p \from V \to W\) of maximal 
degree \(N\) and sets \(V, W \subseteq \C^K,\ K \in \N\) we use the notation 
\(p \in \pols[N]{V, W}\). \jakob{I mean this is the general notation but do we 
even have polynomials that map to $\C^K$.}
We use \(f \propto g\) for functions \(f, g\) to denote \(f = 
c g\) for a constant \(c \in \R\) and \(f \lesssim g\) to denote \(f \leq c 
g\). With \(B(X)\) we denote the closed unit ball of a normed space \(X\), 
i.e.\ \(B(X) \defn \set{x \in X: \norm[X]{x} \leq 1}\). With \(X^\star\) we 
denote the space of linear, bounded functionals on \(X\), i.e. \(X^\star \defn 
\set{x^\star\from X \to \R \mid x^\star \text{ is linear and bounded}}\).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Smolyak's Algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TODO[jakob]: We need citations here
Smolyak's Algorithm is determinstic and can be used in various settings. These include but are not limited to
\begin{enumerate}
    \item function interpolation,
    \item numerical integration,
    \item solving ODEs and PDEs,
    \item \(\cdots\)
\end{enumerate}
\subsection*{Interpolation in one dimension}
In the onedimensional setting it is well known that for a given function 
$f\from\R\to\R$ and a point collection \(\ZZ 
\defn \set{z_n}_{n=0}^N \subseteq \R\), containing of $N+1$ different points, 
there exists an interpolating polynomial $i\in\pols[N]{\R,\R}$ such that for 
all $j\in\indexset[0]{N}$ we have $i(z_j) = f(z_j)$.
To find the polynomial \(i\), one can, among other available methods, choose to 
perform Lagrange Interpolation which yields an explicit construction of the form
\begin{equation}\label{eq:lagrangeInterpolation}
    \mathcal{I}\left(\FF\right)(x) = \sum_{n=0}^N f(z_n) \ell_n(x)\quad \text{ 
    s.t. }\quad \ell_n(x) \defn \prod_{\substack{j = 0\\j \neq n}}^N 
    \frac{x-z_j}{z_n-z_j}
\end{equation}
where \(\FF \defn \set{\left[z_n, f\left(z_n\right)\right]}_{n=0}^{N}\). We set 
\(i \defn \cI(\FF)\). Note that \(\cI\) is a mapping from data, which may be 
seen as arranged in a \(2 \times N\) matrix, to an \(N\)-degree polynomial. 
Thus \(\cI\from \R^{2 \times N} \to \pols[N]{\R,\R}\), since each basis 
function \(\ell_n(x)\) is a polynomial of degree \(N\). Therefore, if \(f \in 
\pols[N]{\R, \R}\), one obtains \(i = f\). Moreover, \(i\) is interpolating 
since \(\ell_n(z_j) = \delta_{n}(j)\). \newline

% TODO: Cite the problems? There are surely papers, where those problems were 
%addressed
% TODO: (3) is almost saying the same as (2)
Even though \Cref{eq:lagrangeInterpolation} has nice theoretical properties, 
it is suboptimal for practical settings due to the following facts. (1) If a 
node \(z_n\) is close to a node \(z_j\) such that \(j \neq n\), computing the 
product \(\ell_n(x)\) becomes numerically unstable. (2) The addition of new 
data \(\left(z, f(z)\right)\) requires to recalculate all basis functions again 
in \(\cO(N^2)\) time. (3) To compute \(i(x)\) in this form requires 
computational complexity of \(\cO(N^2)\). This does not mean that one has to 
abandon Lagrange Interpolation at once. We may write 
\Cref{eq:lagrangeInterpolation} in a much more stable way as
\begin{equation}\label{eq:barycentricLagrange}
    i(x) \defn \cI(\FF)(x) = \frac{\sum_{n=0}^N \frac{w_n}{x-z_n} f(z_n)}{\sum_{n=0}^N \frac{w_n}{x-z_n}}
\end{equation}
for \(w_n \in \R : n \in \indexset[0]{N}\). If the weights \(w_n\) are known, 
computing 
\(i(x)\) in this form admits a complexity of \(\cO(N)\). Luckily, the weights 
have a closed-form, analytic solution for many deterministic point sets used in 
practice and are hence considered to be computable in \(\cO(1)\) time. Thus, 
adding a new data pair \((z, f(z))\) requires total complexity of \(\cO(N)\) 
for the recomputation of said weights. The general identity
 \[
    w_n = \frac{1}{\ell'(x_n)}
\]
always holds. A survey on the \emph{barycentric} form of Lagrange interpolation can be found in \cite{Berut_2004}. To specify the above, consider the set of \emph{equidistant} 
nodes \(\set{z_n \defn 2/n}_{n=0}^{N} \subset [-1, 1]\). Then 
\begin{equation}\label{eq:weightsEquidistant}
    w_n = {\left(-1\right)}^n \binom{N}{n}.
\end{equation}
For large \(N\), \Cref{eq:weightsEquidistant} is problematic since weights 
\(w_i, w_j: i \neq j\) now may vary by factors as large as 
\(\cO\left(2^N\right)\). This means that interpolation in equispaced points is 
susceptible to the \emph{Runge phenomenon}. For this interpolation problem to 
be well posed one should use point sets with asymptotic density \(\rho(x) 
\propto 1/\sqrt{1-x^2}\). This is called the \emph{Chebyshev} density.\newline 
Many point sets admit this asymptotic density, for 
example the Chebyshev points of the first kind are given as the roots of the 
$n+1$-th Chebyshev polynomial and thus given by
% TODO: Note that they're defined differently if we only need n points. 
%TODO: We would have \cos \frac{(2k+1)\pi}{2n} for that
\begin{equation}\label{eq:chebyPts1}
    \ZZ_C^{(1)}(N) \defn 
    \set{\cos \frac{\left(2n+1\right) \pi}{2N+2} \mid n \in 
    \indexset[0]{N}}.
\end{equation}
Similarly, the Chebyshev points of the second kind are defined
\begin{equation}\label{eq:chebyPts2}
    \ZZ_C^{(2)}(N) \defn
    \set{\cos \frac{n\pi}{N} \mid n \in \indexset[0]{N}}.
\end{equation}
In either of these cases, the weights \(w_n\) admit nice closed forms as \(w_n^{(k)}: k \in \set{1,2}\) dependent on which point set is in use as
\begin{equation}\label{eq:weightsChebyshev}
    w_n^{(1)} = {\left(-1\right)}^n \sin \frac{\left(2n+1\right) \pi}{2n+2}, \quad w_n^{(2)} = {\left(-1\right)}^n \cdot \begin{cases}
        1/2 &n \in \set{0, N}\\
        1 &\text{else.}
    \end{cases}
\end{equation}

\subsection*{Interpolation in \(d\) dimensions} In principle, one may just 
\emph{extend} one dimensional interpolation rules to \(d\) dimensions by 
forming the tensor product of all interpolation rules. To specify, take the one 
dimensional algorithm
% TODO: Notation passt nicht ganz zu vorher zusammen. 1d Lagrange hatte Punkte 
% TODO: x_i mit i=0,...,n. Hier haben wir 1-m(N)
\begin{equation}\label{eq:general1dInterpolation}
    \cI^{(N)}(\FF(N))(x) = \sum_{n = 1}^{m(N)} b_{n}(x) f(z_n)
\end{equation}
% TODO: Allow m\from\N_0\to\N_0 or do we need >0?
such that \(\cI^{(N)}(\FF(N))\) is interpolating on the data \(\FF(N) \defn 
\set{\left[z_n, f\left(z_n\right)\right]}_{n=1}^{m(N)}\) with basis functions 
\(b_n \in C[0, 1]\) by having \(b_n(z_j) = \delta_n(j)\). Note that 
$m\from\N_0\to\N_0$ is a common abstraction for the number of points used to 
interpolate a given function. In the case of special interpolation methods or 
their corresponding underlying point set, $m$ will be specified.\newline
To obtain a set of $d$-dimensional point sets from $d$ such one-dimensional 
rules like \Cref{eq:chebyPts1} or \Cref{eq:chebyPts2}, one uses the usual 
cartesian product % TODO: I don't think we get all combinations out of this 
%formula, therefore I adapted it a bit
%\begin{equation}\label{eq:cartProduct}
%    \ZZ(N, d) \defn \ZZ(N_1) \times \cdots \times \ZZ(N_d) \defn 
%    \bigcup_{j=1}^{d} \bigcup_{n_j=1}^{m(N_j)} \set{\left(z_{n_1}, \dots, 
%    z_{n_d}\right)}
%\end{equation}
\begin{equation}\label{eq:cartProduct}
	\ZZ(N, d) \defn \ZZ(N_1) \times \cdots \times \ZZ(N_d) \defn 
	\bigcup_{n_1=1}^{m(N_1)} \cdots \bigcup_{n_d=1}^{m(N_d)} 
	\set{\left(z_{n_1}^{(1)}, \dots, z_{n_d}^{(d)}\right)}
\end{equation}
% Here again: Is N\in\N_0^d or \in \N^d?
where $z_j^{(i)}$ denotes the $j$-th point of the $i$-th rule and \(N \defn 
\left(N_1, \dots, N_d\right) \in \N_0^d\) and $z_n\defn (z_{n_1}, \ldots, 
z_{n_d})$ as the usual notation for multi-indices. Using the 
tensor product formula, one obtains the very general expression
\begin{equation}\label{eq:tensorProdInterp}
    {\left( \cI^{(N_1)} \otimes \cdots \otimes \cI^{(N_d)} \right)}(\FF)
    = \sum_{n_1 = 1}^{m(N_1)} \cdots \sum_{n_d = 1}^{m(N_d)} \left( b_{n_1} 
    \otimes \cdots \otimes b_{n_d} \right) f(z_{n})
\end{equation}
where \(\FF \defn \bigcup_{j=1}^d \FF(N_j)\).  % TODO: This is wrong. If we 
%would only take the union, we still have 1d points. We need the cartesian 
%product somehow
In this case, the tensor product 
interpolation is a mapping of the form
\begin{equation}\label{eq:tensorProdInterpMappingSpecification}
    \bigotimes_{j=1}^d \cI^{(N_j)}\from \R^{d \times \left(\prod_{j=1}^d 
    m(N_j) \right)} \to \pols[\left(\max \set{m(N_j)} \right)]{\R^d, \R}.
\end{equation}
We denote \(\cI^{(N)} \defn \bigotimes_{j=1}^d \cI^{(N_j)}\). If the basis functions \(b_n\) are scalar-valued, their tensor product is just
\begin{equation}\label{eq:tensorProdFunc}
    \left( \bigotimes_{n=1}^d b_n \right)(x_1, \dots, x_d) \defn \prod_{n \in [d]} b_n(x_n).
\end{equation}
% TODO: New Section? Or at least mention what Smolyak is
In Smolyak's Algorithm, the function \(m\from \N_ \to \N_0\) is used as a 
growth 
rule specifying how many points are to be used by the interpolant. 
Specifically, we will use a doubling rule such that
\[
    m(n) \defn \begin{cases}
        2^{n-1}+1 &n \geq 2\\
        1 &\text{else}.
    \end{cases}
\]
Clearly, the usual representation of the Lagrange polynomial 
\Cref{eq:lagrangeInterpolation} fits this framework. For the barycentric form, 
see \cite{Berut_2004}, one obtains the \(d\)-variate interpolation
\begin{equation}\label{eq:multivarBarycentric} % TODO: Don't know where this 
%comes from
    \cI(\FF)(x) \defn \frac{\sum_{n_1=1}^{m(N_1)} b^{(1)}_{n_1}(x_1) \cdots \sum_{n_d=1}^{m(N_d)}b^{(d)}_{n_d}(x_d) f(z_{n_1}, \dots, z_{n_d})}{\sum_{n_1=1}^{m(N_1)} b^{(1)}_{n_1}(x_1) \cdots \sum_{n_d=1}^{m(N_d)}b^{(d)}_{n_d}(x_d)}
\end{equation}
where
\begin{equation}\label{eq:barycentricBasis}
    b^{(k)}_{n_k}(x) \defn \frac{w_{n_k}}{x-z_{n_k}} \mid k \in [d].
\end{equation}
% TODO: We have never defined when a point se is admissible. (No point twice 
%etc.)
If a rule yielding admissible point sets, like Chebyshev's rules 
\Cref{eq:chebyPts1} or \Cref{eq:chebyPts2}, is available, the only thing 
left to do is to specify how many nodes $m(N_j)$ to pick in each 
dimension. If this number is large simultaneously for all dimensions,
\Cref{eq:tensorProdInterp} becomes intractable very quickly. Smolyak's 
algorithm defines a resolution-like number \(q \in \N\) to specify how 
close-meshed points should be picked. It proceeds by taking all multi-indices 
\(N\) with \(1\)-norm less equal to \(q\) and specifies to interpolate 
with the algorithm % TODO: Cite "Smolyak" or is this already "common" knowledge?
\begin{equation}\label{eq:smolyak}
    \cA(q, d)\left(\XX\right) \defn \sum_{\norm[1]{N} \leq q} \cI^{(N)}(\XX(N)).
\end{equation}
Note that, with this construction \(q \geq d\) is necessary. The set \(\XX\) in 
this case is % TODO: Again wrong because taking the union of subsets of \R 
% TODO: is still 1d
\begin{equation}\label{eq:indexGrid}
    \XX(N) \defn \bigcup_{n \in [d]} \FF(N_n)
\end{equation}
All points used by this algorithm are then
\begin{equation}\label{eq:sparseGrid}
    \XX \defn \XX^{\leq}(q) \defn \bigcup_{\norm[1]{N} \leq q} \XX(N).
\end{equation}
% TODO: When is a point set "nice"?
% TODO: Again, \N_0 instead of \N
We call \(\XX\) a \emph{sparse grid}. We use \(\XX^{=}(q)\) for denoting the 
corresponding set replaced with the rule that it takes all vectors \(N_0\in\N^d 
\) with \(\norm[1]{N} = q\). In case the point set is nice, for example for 
Chebyshev points, one obtains a \emph{nesting} of sparse grids for increasing 
fineness scales: \(\XX^{\leq}(q) \subset \XX^{\leq}(q+1)\). This construction 
has the advantage that in the case of applying this algorithm on a computer, 
storing whole grid $\XX$ in the memory is not necessary. It is enough to use 
the refinements of each level-increase \(q \mapsto q+1\) and interpolate at 
each such level separately, each time discarding the 
old values. We may thus define the difference operator
\begin{equation}\label{eq:differenceOp}
    \Delta^{(q)} \defn \left( \bigotimes_{\norm[1]{N}=q+1} \cI^{(N)} \right) - \left( \bigotimes_{\norm[1]{N} = q} \cI^{(N)} \right)
\end{equation}
Now, Smolyak's algorithm can be written as
\begin{equation}\label{eq:SmolyakWithDifferences1}
    \cA(q, d) \defn \sum_{\norm[1]{N} \leq q} \Delta^{(q)}(\XX(N))
\end{equation}
or equivalently
\begin{equation}\label{eq:SmolyakWithDifferences2}
    \cA(q, d) = \cA(q-1, d) + \sum_{\norm[1]{N}=q} \cI^{(N)}(\XX(N))
\end{equation}
with \(\cI^{(0)} = 0\).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Least Squares}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% TODO: Bad notation, as we are using $^\star$ for the optimal solution and
% TODO: We are using $^\star$ for the functionals

Contrary to the construction of exactly interpolating approximants in the case of Smolyak's algorithm, least squares is a conceptually simpler algorithm. We are given the overdetermined system \[
    Vz = y
\]
where \(V \in \R^{n \times m}\) with \(n > m\). It is well--known that this system may be inconsistent and no exact solution exists. However, one may always pose the optimisation problem solving for \(z \in \C^{m}\) with the smallest error
\begin{equation}\label{eq:leastSquares}
    \inf_{z \in \R^{m}} \norm[]{Vz - y}.
\end{equation}
It is further known that, in case of a full--rank matrix \(V\), the unique 
solution to \Cref{eq:leastSquares} is given by 
\begin{equation}\label{eq:ls_solution_moore_penrose_inverse}
	z^\star = \left(V^\top V\right)^{-1} V^\top y \in \R^m.
\end{equation}
    
% TODO: Describe that we are using Chebyshev Polynomials, i.e. the exact same 
% polynomials as Smolyak
In our specific case of polynomial interpolation, \(V\) is the Vandermonde 
matrix, consisting of basis polynomials \(b_1, b_2, \dots, b_m\) evaluated at 
the $n$ different sampled points \(x_1, x_2, \dots, x_n\) in \(\Omega \subseteq 
\R^d\) and \(y\) is the vector of function values sampled from the unknown 
function \(f\from \Omega \to \R\), i.e.\ \(y = \left(f(x_j)\right)_{j=1}^n\). 
As for such points, the Vandermonde matrix is never singular, the solution to 
the approximation problem \[
    \inf_{p} \norm[]{f-p}
\]
can analytically be expressed as \[
    p^\star\from \Omega \to \R, t \mapsto \sum_{j=1}^m z_j^\star b_j(t)
\]
where \(z^\star = \left(z_j\star\right)_{j=1}^m\) is given by 
\Cref{eq:ls_solution_moore_penrose_inverse}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Theoretical Guarantees}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The notation in the following is borrowed from \cite{Ullrich_2020}.
In this section we introduce a formal setting to the former considerations. That is, we consider a Hilbert space \(H\) of real-valued functions on a set \(D\) such that point evaluation \[
    \delta_x: f \mapsto \int_D f\ \d \delta_x = f(x)
\]
is a bounded, linear functional on \(H\).
%TODO: Mention >>Weighted<< Least Squares?
The general formulation of least squares allows for a broad class of recovery problems. In our specific case, the function recovery of real--valued functions on a \(d\)--dimensional (compact) subset \(D\) using basis functions of a \(k\)--dimensional subspace \(V_k \defn \spn \set{b_1, \dots, b_k}\), we consider the specific form of least squares, given by\[
    A_{n, k}(f) \defn \argmin_{g \in V_k} \sum_{i=1}^n \frac{\abs{g(x_i) - f(x_i)}^2}{\varrho_k(x_i)}
\]
where \[
    \varrho_k(x) = \frac{1}{2} \left( \frac{1}{k} \sum_{j < k} b_{j+1}(x)^2 + \frac{1}{\sum_{j \geq k} a_j^2} \sum_{j \geq k} a_j^2 b_{j+1}(x)^2 \right)
\]
and \(x_1, \dots, x_n \in D\). Whenever \(f \in V_k\), then, of course, \(f = A_{n, k}(f)\). With
\begin{equation}\label{eq:worstCaseError}
    e\left(A_{n,k}, H\right) \defn \sup_{f \in B(H)} \norm[L_2]{f - A_{n,k}(f)},
\end{equation}
we denote the worst case error of \(A_{n,k}\), where we measure the error of the reconstruction in the space \(L_2 \defn L_2(D, \Sigma, \mu)\) of square integrable functions on \(D\) with respect to the measure \(\mu\), such that \(H\) is embedded into \(L_2\). In light of this, the \(n\)--th minimal error is denoted by \[
    e_n(H) \defn \inf_{\substack{x_1, \dots, x_n \in D \\ \phi_1, \dots, \phi_n 
    \in L_2}} \sup_{f \in B(H)} \norm[L_2]{f - \sum_{i=1}^n f(x_i) \phi_i}
\]
and can be understood as the worst case error of the optimal algorithm using \(n\) function values. We get the clear inequality \(e_n(H) \leq e(A_{n,k}, H)\) for any point set \(\set{x_1, \dots, x_n}\). With \[
    a_n(H) \defn \inf_{\substack{h_1^\star, \dots, h_n^\star \in H^\star \\ 
    \phi_1, \dots, \phi_n \in L_2}} \sup_{f \in B(H)} \norm[L_2]{f - 
    \sum_{i=1}^n h_i^\star(f) \phi_i}
\]
we denote the \(n\)-th approximation number, which is the worst-case error of 
an optimal algorithm that uses the \(n\) best arbitrary linear and bounded 
functionals as information about the unknown. This quantity is equal to the 
\(n\)-th singular value of the embedding \(\id\from H \to L_2\).

The following is known since \cite{Krieg_2020}.
\begin{thm}[Krieg--Ullrich]\label{thm:kriegUllrich2020}
    There exist constants \(C, c > 0\) and a sequence of natural numbers \((k_n)\) with each \(k_n \geq cn/\log(n+1)\) and for any \(n \in \N\) and measure space \((D, \Sigma, \mu)\), and any RKHS \(H\) of real-valued functions on \(D\) embedded into \(L_2(D, \Sigma, \mu)\), we have \[
        e_n(H) \leq \sqrt{\frac{C}{k_n} \sum_{j \geq k_n} a_j(H)^2}.
    \]
\end{thm}
In particular, for 
\begin{equation}\label{eq:orderOfApproximationNumbers}
    a_n(H) \lesssim n^{-s} \log^{\alpha + s}(n)
\end{equation}
with \(s > 1/2, \alpha \in \R\), this implies \[
    e_n(H) \lesssim n^{-s} \log^{\alpha+s}(n).
\]
The following follows from \cite{Ullrich_2020}.
\begin{thm}[Ullrich]
    Given \(n \geq 2\) and \(c > 0\), let \[
        k_n \defn \floor*{\frac{n}{2^8 (2+c) \log n}},
    \]
    then, for any measure space \((D, \Sigma, \mu)\) and any RKHS \(H\) of real-valued functions on \(D\), embedded into \(L_2(D, \Sigma, \mu)\), it holds that \[
        e_n\left(A_{n, 2 k_n}, H\right) \leq \sqrt{ \frac{2}{k_n} \sum_{j > k_n} a_j(H)^2 }
    \]
    with probability at least \(1-8n^{-c}\).
\end{thm}

\subsection*{Examples}
In particular, \Cref{eq:orderOfApproximationNumbers} is satisfied for the 
approximation numbers on the Sobolev space of dominating mixed smoothness, 
\begin{align*}
    H &\defn H_{\text{mix}}^s\left(\T^d\right)\\
    &\defn \set{f \in L_2\left(\T^d\right) : \norm[H]{f}^2 \defn \sum_{m \in 
    \N_0^d} \prod_{j=1}^d (1+\abs{m_j}^{2s}) \inner{f}{b_m}^2_{L_2} < \infty}
\end{align*}
where \(b_m \defn \otimes_{j=1}^d b_{m_j}^{(1)}\) and \(m = (m_1, \dots, m_d)\) with \begin{align*}
    b_{2m}^{(1)} &\defn \sqrt{2}\cos(2\pi m x)\\
    b_{2m-1}^{(1)} &\defn \sqrt{2}\sin(2\pi m x)
\end{align*}
and \(b_0^{(1)} \defn 1\). This satisfies the assumption for \(s > 1/2\). In 
particular, we can say
\begin{equation}\label{eq:conclusio}
    e_n\left( H_{\text{mix}}^s\left(\T^d\right) \right) \lesssim n^{-s}\log^{sd}(n)
\end{equation}
whenever \(s > 1/2\). This disproves a previously posted conjecture (Conjecture 
5.26) in \cite{Dung_Temlyakov_Ullrich_2018} and shows that Smolyak's algorithm 
is not optimal in this case. The surprising fact is that, despite an optimal, 
deterministic construction of the point sets used for reconstruction being 
unknown, random i.i.d. points suffice for a reconstruction error that is on the 
order of optimal points, with probability tending to \(1\). We verify this by 
our experimental findings, presented in \Cref{sec:experimentalFindings} with a 
much better relative number of points used for LSQ function recovery vs. SA 
recovery than guaranteed in this section, i.e. better constants than explicitly 
known before. It remains an open problem to rigorously improve upon the 
constants in \Cref{eq:conclusio}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Findings}\label{sec:experimentalFindings}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage

\nocite{*}
\printbibliography

\bigskip

\noindent
\address{J.E., Johannes Kepler University Linz; 
\texttt{jakob.eggl@jku.at}; \\
	E.M., Johannes Kepler University Linz; 
	\texttt{elias.mindlberger@jku.at}; \\
	M.U., Johannes Kepler University Linz; 
	\texttt{mario.ullrich@jku.at}
}

\end{document}
